{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a51b026-642a-4de8-8945-35654584d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import itertools\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b245b4-b54c-434b-b883-4168fd901a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "\n",
    "stopwords_path = 'dataset/stopword-id.csv'\n",
    "acronym_path = 'dataset/acronym.csv'\n",
    "out_dir = \"docs\"\n",
    "out_vocab_path = \"docs/vocab.json\"\n",
    "out_idf_path = \"docs/idf.f32\"\n",
    "\n",
    "# Actual news data. Suit your need\n",
    "csv_path = \"../data/final_merge_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a2c1ffa-a46c-4fec-804d-3a6873f86186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "stopwords_df = pd.read_csv(stopwords_path, header=None)\n",
    "custom_stopwords = stopwords_df[0].tolist()\n",
    "\n",
    "# Load acronyms and build replacement dictionary\n",
    "df_acronym = pd.read_csv(acronym_path)\n",
    "acronym_dict = dict(zip(df_acronym[\"acronym\"], df_acronym[\"expansion\"]))\n",
    "acronym_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, acronym_dict.keys())) + r')\\b')\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    \n",
    "    # Replace acronyms\n",
    "    text = acronym_pattern.sub(lambda match: acronym_dict[match.group(0)], text)\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove HTML image tags\n",
    "    text = re.sub(r'<img[^>]*>', '', text)\n",
    "    \n",
    "    # Remove mentions, URLs, numbers\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Clean punctuation and excess whitespace\n",
    "    text = text.replace(\"b'\", \"\").replace(\"-\", \" \")\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove specific unwanted terms\n",
    "    text = text.replace(\"img\", \"\").replace(\"src\", \"\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d21625-7f04-44bf-8103-7c1747b062fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV into DataFrame\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35ce6105-9d9b-4b53-983d-50d7a7ce200e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Judul', 'Waktu', 'Link', 'Content', 'tag1', 'tag2', 'tag3', 'tag4',\n",
       "       'tag5', 'source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20385450-da09-4b7c-9d4f-66f80c1b2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS\n",
    "\n",
    "df[\"preprocessed_text\"] = df[\"Judul\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11ba30ba-2259-4b97-89fa-025f40f48f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Judul', 'Waktu', 'Link', 'Content', 'tag1', 'tag2', 'tag3', 'tag4',\n",
       "       'tag5', 'source', 'preprocessed_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85c690fa-1189-4b25-97bf-2f494a333760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZE\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=custom_stopwords, max_features=1000)\n",
    "vec = vectorizer.fit_transform(df['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a8aa7c3-f93f-416f-a68c-22588ce560b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cabf8a84-9cf9-4fc1-998f-a54031998229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'viral': np.int64(980),\n",
       " 'isu': np.int64(319),\n",
       " 'hubungan': np.int64(284),\n",
       " 'kerja': np.int64(406),\n",
       " 'buruh': np.int64(142),\n",
       " 'pemilik': np.int64(637),\n",
       " 'terdampak': np.int64(911),\n",
       " 'gempa': np.int64(249),\n",
       " 'masehi': np.int64(513),\n",
       " 'guncang': np.int64(261)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(itertools.islice(vocab.items(), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d337cec0-e0ac-4b22-b4a2-a4277dc59525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert np.int64 â†’ int\n",
    "vocab_clean = {term: int(idx) for term, idx in vocab.items()}\n",
    "with open(out_vocab_path, \"w\") as f:\n",
    "    json.dump(vocab_clean, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9f50a52-8261-41a3-8417-595a88179a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved idf.f32 with shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# vectorizer is your fitted TfidfVectorizer\n",
    "idf = vectorizer.idf_.astype(\"float32\")\n",
    "\n",
    "# write to a raw Float32 binary file\n",
    "idf.tofile(out_idf_path)\n",
    "\n",
    "print(\"Saved idf.f32 with shape:\", idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d329806f-d6a0-4313-b3b7-6667f02dd4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (1000,)\n",
      "[7.183689  6.601945  6.8929996 7.397837  6.179785  5.15681   6.3767834\n",
      " 6.8795767 6.366088  7.038182 ]\n",
      "[7.18368922 6.60194486 6.89299962 7.3978372  6.17978488 5.15681\n",
      " 6.37678315 6.8795766  6.36608786 7.03818163]\n"
     ]
    }
   ],
   "source": [
    "x = np.fromfile(out_idf_path, dtype=np.float32)\n",
    "\n",
    "print(\"Loaded:\", x.shape)\n",
    "print(x[:10])   # first 10 for checking\n",
    "print(vectorizer.idf_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "affa2fea-3e49-4ee9-8be4-fbc0917aa8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80472, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "973376d3-3410-4ddf-846b-13022db700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing shard 0: vectors 0..8387 -> vectors_000.f32 (8388 vectors)\n",
      "Writing shard 1: vectors 8388..16775 -> vectors_001.f32 (8388 vectors)\n",
      "Writing shard 2: vectors 16776..25163 -> vectors_002.f32 (8388 vectors)\n",
      "Writing shard 3: vectors 25164..33551 -> vectors_003.f32 (8388 vectors)\n",
      "Writing shard 4: vectors 33552..41939 -> vectors_004.f32 (8388 vectors)\n",
      "Writing shard 5: vectors 41940..50327 -> vectors_005.f32 (8388 vectors)\n",
      "Writing shard 6: vectors 50328..58715 -> vectors_006.f32 (8388 vectors)\n",
      "Writing shard 7: vectors 58716..67103 -> vectors_007.f32 (8388 vectors)\n",
      "Writing shard 8: vectors 67104..75491 -> vectors_008.f32 (8388 vectors)\n",
      "Writing shard 9: vectors 75492..80471 -> vectors_009.f32 (4980 vectors)\n",
      "Done. Manifest saved to docs/manifest.json\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "N, DIM = vec.shape\n",
    "BYTES_PER_VEC = DIM * 4\n",
    "TARGET_SHARD_BYTES = 32 * 1024 * 1024  # 32 MB target\n",
    "vecs_per_shard = max(1, TARGET_SHARD_BYTES // BYTES_PER_VEC)\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "manifest = {\n",
    "    \"total_vectors\": int(N),\n",
    "    \"dim\": int(DIM),\n",
    "    \"dtype\": \"float32\",\n",
    "    \"header_size\": 16,\n",
    "    \"shards\": []\n",
    "}\n",
    "\n",
    "def write_header(f, num_vectors, dim, dtype_code=1):\n",
    "    # magic + num_vectors + dim + dtype_code, all little-endian uint32\n",
    "    f.write(b'VECT')  # 4 bytes\n",
    "    f.write(np.uint32(num_vectors).tobytes())\n",
    "    f.write(np.uint32(dim).tobytes())\n",
    "    f.write(np.uint32(dtype_code).tobytes())\n",
    "\n",
    "start = 0\n",
    "shard_idx = 0\n",
    "while start < N:\n",
    "    end = min(N, start + vecs_per_shard)\n",
    "    count = end - start\n",
    "    shard_name = f\"vectors_{shard_idx:03d}.f32\"\n",
    "    shard_path = os.path.join(out_dir, shard_name)\n",
    "    print(f\"Writing shard {shard_idx}: vectors {start}..{end-1} -> {shard_name} ({count} vectors)\")\n",
    "    with open(shard_path, \"wb\") as f:\n",
    "        write_header(f, count, DIM, dtype_code=1)\n",
    "        # Convert only this slice to dense float32 (row-wise)\n",
    "        # For memory safety, do it in smaller sub-batches if needed\n",
    "        batch_size = 1024  # adjust if your memory is tight\n",
    "        for bstart in range(start, end, batch_size):\n",
    "            bend = min(end, bstart + batch_size)\n",
    "            dense = vec[bstart:bend].toarray().astype(np.float32)\n",
    "            f.write(dense.tobytes(order='C'))\n",
    "    shard_info = {\n",
    "        \"id\": shard_idx,\n",
    "        \"shard\": shard_name,\n",
    "        \"url\": f\"/{shard_name}\",  # set as you will serve it\n",
    "        \"start_index\": int(start),\n",
    "        \"count\": int(count),\n",
    "        \"size_bytes\": os.path.getsize(shard_path)\n",
    "    }\n",
    "    manifest[\"shards\"].append(shard_info)\n",
    "    start = end\n",
    "    shard_idx += 1\n",
    "\n",
    "manifest_path = os.path.join(out_dir, \"manifest.json\")\n",
    "with open(manifest_path, \"w\", encoding=\"utf-8\") as mf:\n",
    "    json.dump(manifest, mf, indent=2)\n",
    "\n",
    "print(\"Done. Manifest saved to\", manifest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a1b3e-8087-480d-aa8a-1aadd67ed769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb6d0ffc-e27f-4753-a988-2b7b31b37e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def generate_metadata_and_index(\n",
    "    df: pd.DataFrame,\n",
    "    output_dir: str = out_dir,\n",
    "    meta_filename: str = \"metadata.jsonl\",\n",
    "    index_filename: str = \"metadata.index\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create metadata.jsonl and metadata.index for HTTP Range Request lookup.\n",
    "    \"\"\"\n",
    "    out = Path(output_dir)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    meta_path = out / meta_filename\n",
    "    index_path = out / index_filename\n",
    "\n",
    "    byte_offset = 0\n",
    "    index_lines = []\n",
    "\n",
    "    with meta_path.open(\"wb\") as f_meta:  # write raw bytes\n",
    "        for _, row in df.iterrows():\n",
    "            record = {\n",
    "                \"title\": row[\"Judul\"],\n",
    "                \"url\": row[\"Link\"],\n",
    "                \"date\": row[\"Waktu\"],\n",
    "            }\n",
    "\n",
    "            json_line = json.dumps(record, ensure_ascii=False) + \"\\n\"\n",
    "            encoded = json_line.encode(\"utf-8\")\n",
    "\n",
    "            # record byte offset\n",
    "            index_lines.append(str(byte_offset))\n",
    "\n",
    "            # write jsonl\n",
    "            f_meta.write(encoded)\n",
    "\n",
    "            # update offset\n",
    "            byte_offset += len(encoded)\n",
    "\n",
    "    # write index file\n",
    "    index_path.write_text(\"\\n\".join(index_lines), encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Created: {meta_path}\")\n",
    "    print(f\"Created: {index_path}\")\n",
    "    print(f\"Total records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3639f982-8b62-4bcd-8b4a-3cf25f741e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: docs/metadata.jsonl\n",
      "Created: docs/metadata.index\n",
      "Total records: 80472\n"
     ]
    }
   ],
   "source": [
    "generate_metadata_and_index(df, output_dir=out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
